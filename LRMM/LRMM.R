### This is a library for Logistic Regression Mixture Model(LRMM)
rdirichlet <- function(phi) {
  ### generate random variable following Dirichlet distribution
  numerator <- rgamma(n = length(phi), shape=phi, rate=1)
  return(numerator / sum(numerator))
}

sigmoid <- function(x,a){
  ### sigmoid function
  return(1 / (1 + exp(-x%*%a)))
}

sigmoid.integral.by.normal <- function(x,weight, Sigma){
  ### This function represents approximated integral \int N(a|weight, Sigma) sigmoid(x,a) da
  n <- dim(x)[1]
  K <- dim(weight)[2]
  mu.a <- x %*% weight
  S.a <- matrix(0, nrow=n, ncol=K)
  for(k in 1:K){
    S.a[,k] <- rowSums((x %*% Sigma[,,k])*x)
  }

  integrated.sigmoid <- 1/(1 + exp(- mu.a / sqrt(1+pi/8*S.a) ))
}

generate.param.logistic.mixture <- function(parameter.num, K, M, seed = 1, hyperparameter = NA){
  ### This function generates true parameter for weight and mixing ratio.
  library(MASS)
  
  set.seed(seed = seed)
  if(!is.list(hyperparameter)){
    hyperparameter = list(phi = rep(1,K),
                          mu = matrix(0, nrow=K, ncol=M),
                          beta = array( apply(array(0,dim=c(M,M,K)), 3, function(x)diag(M)), dim=c(M,M,K) ))
  }
  weight <- array(0, dim=c(parameter.num, M, K))
  
  ratio <- matrix( apply(matrix(0, nrow=parameter.num, ncol=K), 1, function(x) rdirichlet(hyperparameter$phi)), nrow=parameter.num, ncol=K, byrow=T)
  for(k in 1:K){
    weight[,,k] <- mvrnorm(n=parameter.num, mu=hyperparameter$mu[,k], Sigma=solve(hyperparameter$beta[,,k]))
  }  
  return( list(ratio = ratio, weight = weight) )
}

rlogistic.mixture <- function(n, base.dim, x.range, input.generator, param, seed = 1, is.label=TRUE){
  ### This function generates input and output distribution p(x,y|w)
  ### p(x) is generated by input.generator,
  ### p(y|x,w) is generated by LRMM with parameters param
  
  set.seed(seed=seed)
  
  ##input
  x.base <- matrix(input.generator(n*M, min = x.range[1], max = x.range[2]), nrow=n, ncol=base.dim)

  K0 <- length(param$ratio)
  
  if(is.label == TRUE){
    label <- apply(rmultinom(n, size=1, prob=param$ratio), 2, which.max)
    mul.x <- diag(x %*% param$weight[,label])
    prob.y <- 1/(1+exp(-mul.x))
  }else{
    label <- -1
    prob.y <- rowSums( matrix(rep(param$ratio,n),nrow=n, ncol=K0, byrow=T) *
                              1/(1+exp(-x %*% param$weight)) )
  }
  
  y <- as.numeric(runif(n) < prob.y)
  
  return(list(label = label, y = y, x = x, prob.y = prob.y))
}

sampling.handmade <- function(x, sample.size, seed=1){
  set.seed(seed)
  total.size <- dim(x)[1]
  if(total.size < sample.size){
    print("Warning: total size of x is less than sample size !!")
    sample.size <- total.size
  }
  picked.data.ind <- order(rdirichlet(rep(1,total.size)), decreasing = T)[1:sample.size]
  return(x[picked.data.ind,])
}

dlatent.posterior.logistic.mixture <- function(x,y,param){
  ### This function calculates latent posterior distribution
  ### Let z be latent variable, then this function calculates p(z^n|x^n,y^n,w)
  ### Note: p(z|x,y,w) = p(y|z,x,w)p(z|x,w)/p(y|x,w),
  ### and p(y|z,x,w) and p(z|x,w) is obtained like ordinal mixture model,
  ### we can get the distribution.
  
  n <- dim(x)[1]
  K <- length(param$ratio)
  
  
  log.joint.complete <- matrix( rep(log(param$ratio),n), nrow=n, ncol=K , byrow = T) + ((y-0.5)*x) %*% param$weight-log(2*cosh(x %*% param$weight/2))
  max.log.joint.complete <- apply(log.joint.complete,1,max)
  norm.log.joint.complete <- log.joint.complete - max.log.joint.complete
  latent.prob <-  exp(norm.log.joint.complete)/matrix(rep(rowSums( exp(norm.log.joint.complete) ),K),nrow=n,ncol=K) 
  
  return(latent.prob)
}

dlogistic.mixture <- function(x,y,param){
  ### This function calculates the value of probability density function
  ### of LRMM with parameter param.
  
  if(is.numeric(y) == FALSE){
    print("Illegal arugument for y!")
    return(NULL)
  }
  n <- length(y)
  K <- length(param$ratio)
  dval <- numeric(n)
  dval.partial <- matrix(0, nrow=n, ncol=K)
  
  ratio <- param$ratio
  weight <- param$weight
  true.ind <- which(y == 1)
  false.ind <- which(y == 0)
  
  dval.partial <- matrix(rep(true.param$ratio, n), nrow=n, ncol=K0, byrow = T) * sigmoid(x, true.param$weight)
  
  if(length(true.ind) == 1){
    dval[true.ind] <- sum(dval.partial[true.ind,])
  }else{
    dval[true.ind] <- apply(dval.partial[true.ind,],1,sum)
  }
  

  if(length(false.ind) == 1){
    dval[false.ind] <- 1 - sum(dval.partial[false.ind,])
  }else{
    dval[false.ind] <- 1 - apply(dval.partial[false.ind,],1,sum)
  }
  
  return(dval)
}

initialize.LRMM.LVA <- function(n, K, M, init.Sigma,
                                             init.phi=1, init.df = 10, seed=1){
  ### This function initializes an approximated posterior distribution obtained by LVA.
  
  library(MASS)
  set.seed(seed=seed)
  
  # for hyperparameter of parameter distribution
  if(length(init.phi) == 1){
    init.phi <- rep(init.phi,K)
  }
  
  if(init.df < M){
    init.df <- M
  }
  phi <- rdirichlet(init.phi)
  beta <- rWishart(K,df=init.df, Sigma=init.Sigma)
  inv.beta <- array(0,dim=c(M,M,K))
  mu <- matrix(0, nrow=M, ncol=K)
  mu.cov <- array(0, dim = c(M,M,K))
  for(k in 1:K){
    inv.beta <- solve(beta[,,k])
    mu[,k] <- mvrnorm(n=1, mu=rep(0,M), Sigma=inv.beta)
    mu.cov[,,k] <- inv.beta + tcrossprod(mu[,k])  
  }
  
  
  # for latent variables
  est.latent.variable <- matrix(0,nrow=n,ncol=K)
  for(i in 1:n){
    est.latent.variable[i,] <- rdirichlet(rep(1,K))
  }
  
  # for auxiliary variables
  est.auxilirary.variable <- matrix(abs(rnorm(n*K)), nrow=n, ncol=K)
  est.sq.g.eta <- sqrt(est.auxilirary.variable)  
  est.v.eta <- -tanh(est.sq.g.eta/2)/(4*est.sq.g.eta)  
  
  return(list(param = list(phi=phi, mu=mu, beta=beta, mu.cov = mu.cov),
              latent = list(latent.variable=est.latent.variable),
              auxilirary = list(auxilirary.variable=est.auxilirary.variable, v.eta=est.v.eta, sq.g.eta=est.sq.g.eta) ))
}

free.energy.LRMM.LVA <- function(x, in.out.matrix, prior.hyperparameter,
                            param.updated.val, latent.updated.val, auxilirary.updated.val){
  ### This function calculates a current free energy for LVA of LRMM
  ### Note: LVA minimizes this values by iterative calculation.
  
  ## transform argument to actual values used here
  post.hyperparameter <- param.updated.val
  log.lantent.variable.partial <- latent.updated.val$log.latent.partial
  latent.variable <- latent.updated.val$latent.variable
  
  auxilirary.variable <- auxilirary.updated.val$auxilirary.variable
  est.v.eta <- auxilirary.updated.val$v.eta
  
  est.mu.cov <- post.hyperparameter$mu.cov
  
  phi <- prior.hyperparameter$phi
  beta <- prior.hyperparameter$beta
  K <- length(post.hyperparameter$phi)
  
  ## latent variable is relatively small, so normailized here.
  max.log.latent.variable.partial <- apply(log.lantent.variable.partial,1,max)
  norm.log.latent.variable.partial <- log.lantent.variable.partial - max.log.latent.variable.partial
  
  energy <- 0
  energy <- energy - sum(log(rowSums(exp(norm.log.latent.variable.partial)))+max.log.latent.variable.partial)
  for(k in 1:K){
    energy <- energy + sum(latent.variable[,k]*(
      digamma(post.hyperparameter$phi[k]) - digamma(sum(post.hyperparameter$phi)) + 
        in.out.matrix %*% post.hyperparameter$mu[,k] + est.v.eta[,k]* auxilirary.variable[,k]
    ))
    
    energy <- energy + determinant(post.hyperparameter$beta[,,k]/(2*pi), logarithm=T)$modulus[1]/2 - t(post.hyperparameter$mu[,k]) %*% post.hyperparameter$beta[,,k] %*% post.hyperparameter$mu[,k]/2
  }
  energy <- energy + lgamma(sum(post.hyperparameter$phi)) - sum(lgamma(post.hyperparameter$phi)) + K*lgamma(prior.hyperparameter$phi) - lgamma(K*prior.hyperparameter$phi) - K*determinant(prior.hyperparameter$beta*diag(M)/(2*pi), logarithm=T)$modulus[1]/2
  return(energy)
}

lva.estimation.main <- function(update.order=c(lva.update.parameters, lva.update.latent.variable, lva.update.auxilirary.variable),
                                x,
                                y,
                                K,
                                prior.hyperparameter,
                                init.Sigma,
                                init.phi = 1,
                                init.df = 10,                                
                                iteration = 1000,
                                restart = 1,                                
                                learning.seed = 1,
                                trace.on = FALSE,
                                save.file = NA
                                ){
  n <- dim(x)[1]
  M <- dim(x)[2]
  
  est.result <- list()
  est.result$energy.trace <- Inf
  
  for(i.restart in 1:restart){
    i.initial.result <- initialize.logistic.mixture.fast(n, K, M, init.Sigma, init.phi, init.df, seed = learning.seed+i.restart)
    
    # i.restart.result <- lva.estimation.learning(update.order, x, y, K, prior.hyperparameter,
    #                                             i.initial.result, iteration, trace.on=trace.on,
    #                                             step.savefile = paste("result", i.restart, "_tmp.rds", sep=""))
    i.restart.result <- lva.estimation.learning(update.order, x, y, K, prior.hyperparameter,
                                                i.initial.result, iteration, trace.on=trace.on,
                                                step.savefile = NA)
    
    print(c(i.restart, i.restart.result$energy.trace[length(est.result$energy.trace)]))
    
    if(i.restart.result$energy.trace[length(i.restart.result$energy.trace)] < est.result$energy.trace[length(est.result$energy.trace)]){
      est.result <- i.restart.result
    }
    
    if(!is.na(save.file)){
      save(i.restart.result, file = paste("result",as.character(i.restart), "_", save.file, sep = ""))
    }
    
  }
  return(est.result)
}

lva.estimation.learning <- function(update.order=c(lva.update.parameters, lva.update.latent.variable, lva.update.auxilirary.variable),
                                    x,
                                    y,
                                    K,
                                    prior.hyperparameter,
                                    initial.result,
                                    iteration = 1000,
                                    tol = 1e-5,
                                    trace.on = FALSE,
                                    step.savefile = NA
                                    ){
  n <- dim(x)[1]
  M <- dim(x)[2]

  in.out.matrix <- matrix(rep((y - 0.5),M),nrow=n,ncol=M) * x
  
  est.result <- initial.result
  if(trace.on == TRUE){
    est.result$energy.trace <- numeric(iteration)-1
  }else{
    est.result$energy.trace <- 0
  }
  
  for(ite in 1:iteration){
    for(j in 1:length(update.order)){
      j.update.result <- update.order[[j]](x, in.out.matrix, K, prior.hyperparameter, est.result)
      est.result[[j.update.result$name]] <- j.update.result$updated.val
    }
    
    if(trace.on == TRUE){
      est.result$energy.trace[ite] <- lva.free.energy.fast(x, in.out.matrix, prior.hyperparameter,
                                                      param.updated.val = est.result$param,
                                                      latent.updated.val = est.result$latent, 
                                                      auxilirary.updated.val = est.result$auxilirary)      
      print(c(ite, est.result$energy.trace[ite]))
    }else{
      print(ite)
    }
    
    if(!is.na(step.savefile)){
      saveRDS(est.result, file = step.savefile)
    }
  }
  if(trace.on == FALSE){
    est.result$energy.trace <- lva.free.energy.fast(x, in.out.matrix, prior.hyperparameter,
                                                    param.updated.val = est.result$param,
                                                    latent.updated.val = est.result$latent, 
                                                    auxilirary.updated.val = est.result$auxilirary) 
  }
  est.result$mean.estimator <- list(ratio=est.result$param$phi/sum(est.result$param$phi), weight=est.result$param$mu)
  return(est.result)
}


lva.update.parameters <- function(x, in.out.matrix, K, prior.hyperparameter, current.est.result){
  ##pre-processing for this function
  n <- dim(x)[1]
  M <- dim(x)[2]
  phi <- prior.hyperparameter$phi
  beta <- prior.hyperparameter$beta
  est.latent.variable <- current.est.result$latent$latent.variable
  est.v.eta <- current.est.result$auxilirary$v.eta
  
  est.beta <- array(0, dim=c(M,M,K))
  est.mu <- matrix(0, nrow=M, ncol=K)
  est.mu.cov <- array(0, dim=c(M,M,K))
  
  est.phi <- colSums(est.latent.variable)+phi
  for(k in 1:K){
    extend.est.v.eta <- -2 * matrix(rep(est.v.eta[,k],M),nrow=n,ncol=M) ## -2 is added to adjust to necessary calculaion
    extend.est.latent.variable <- matrix(rep(est.latent.variable[,k],M), nrow=n, ncol=M)
    est.beta[,,k] <- crossprod(x, (extend.est.latent.variable * extend.est.v.eta) * x) + beta*diag(M)
    
    inv.est.beta <- solve(est.beta[,,k])
    
    est.mu[,k] <- inv.est.beta %*% colSums(extend.est.latent.variable * in.out.matrix)
    
    est.mu.cov[,,k] <- inv.est.beta + tcrossprod(est.mu[,k])
  }
  
  return(list(name="param", updated.val=list(phi=est.phi, beta=est.beta, mu=est.mu, mu.cov = est.mu.cov)))
}

lva.update.latent.variable <- function(x, in.out.matrix, K, prior.hyperparameter, current.est.result){
  ##pre-processing for this function
  n <- dim(x)[1]
  M <- dim(x)[2]
  est.phi <- current.est.result$param$phi
  est.mu <- current.est.result$param$mu
  est.beta <- current.est.result$param$beta
  
  est.mu.cov <- current.est.result$param$mu.cov
  
  est.v.eta <- current.est.result$auxilirary$v.eta
  est.sq.g.eta <- current.est.result$auxilirary$sq.g.eta
  est.g.eta  <- current.est.result$auxilirary$auxilirary.variable
  
  est.h.xi <- matrix(0, nrow=n, ncol=K)
  for(k in 1:K){
    est.h.xi[,k] <- digamma(est.phi[k]) - digamma(sum(est.phi)) +
      in.out.matrix %*% est.mu[,k] - log(2*cosh(est.sq.g.eta[,k]/2)) + est.v.eta[,k]*(rowSums((x %*% est.mu.cov[,,k]) * x) - est.g.eta[,k])
    # est.h.xi[,k] <- digamma(est.phi[k]) - digamma(sum(est.phi)) +
    #   in.out.matrix %*% est.m[,k] - log(2*cosh(sqrt(est.g.eta[,k])/2))
  }
  max.est.h.xi <- apply(est.h.xi,1,max)
  exp.norm.est.h.xi <- exp(est.h.xi - max.est.h.xi)
  est.latent.variable <-  exp.norm.est.h.xi / matrix(rep(rowSums(exp.norm.est.h.xi),K),nrow=n,ncol=K)
  return(list(name="latent", updated.val=list(latent.variable=est.latent.variable, log.latent.partial=est.h.xi)))
}

lva.update.auxilirary.variable <- function(x, in.out.matrix, K, prior.hyperparameter, current.est.result){
  ##pre-processing for this function
  n <- dim(x)[1]
  
  est.mu.cov <- current.est.result$param$mu.cov
  est.g.eta <- matrix(0, nrow=n, ncol=K)
  for(k in 1:K){
    est.g.eta[,k] <- rowSums((x %*% est.mu.cov[,,k]) * x)
  }
  est.sq.g.eta <- sqrt(est.g.eta)  
  est.v.eta <- -tanh(est.sq.g.eta/2)/(4*est.sq.g.eta)
  
  
  return(list(name="auxilirary", updated.val=list(auxilirary.variable=est.g.eta, v.eta=est.v.eta, sq.g.eta=est.sq.g.eta)))
}

prediction.logistic.mixture.monte <- function(x, parameter.num=10000, estimation.result){
  ### This function creates a predictive distribution for LRMM
  ### Since integration for sigmoid function does not have analytical form,
  ### We adopt the Monte carlo interation for this function.
  library(MASS)
  ratio <- estimation.result$mean.estimator$ratio
  n <- dim(x)[1]
  M <- dim(x)[2]
  K <- length(ratio)
  
  posterior.weight <- array(0, dim=c(parameter.num, M,K))
  for(k in 1:K){
    posterior.weight[,,k] <- mvrnorm(n=parameter.num, mu=estimation.result$param$mu[,k], Sigma=solve(estimation.result$param$beta[,,k]))
  }
  
  pred.sigmoid.func <- monte.carlo.integral.sigmoid(x, posterior.weight)
  pred.prob <- apply(matrix(rep(ratio, n), nrow=n, ncol=K, byrow=T) * pred.sigmoid.func, 1, sum)
  return(pred.prob)
}

prediction.logistic.mixture.approx <- function(x, estimation.result){
  ### This function creates predictive distribution.
  ### Since the integration for sigmoid has no analytical form,
  ### we adopt an approximated interation (c.f. Bishop book).
  ratio <- estimation.result$mean.estimator$ratio
  n <- dim(x)[1]
  M <- dim(x)[2]
  K <- length(ratio)
  
  inv.beta <- array(apply(estimation.result$param$beta, MARGIN = 3, solve), dim=c(M,M,K))
  pred.sigmoid.func <- sigmoid.integral.by.normal(x, estimation.result$param$mu, inv.beta)
  
  pred.prob <- apply(matrix(rep(ratio, n), nrow=n, ncol=K, byrow=T) * pred.sigmoid.func, 1, sum)
  return(pred.prob)
}

prediction.logistic.mixture.prob.fast <- function(x, estimation.result){
  ratio <- estimation.result$mean.estimator$ratio
  n <- dim(x)[1]
  M <- dim(x)[2]
  K <- length(ratio)

  inv.beta <- array(apply(estimation.result$param$beta, MARGIN = 3, solve), dim=c(M,M,K))
  pred.sigmoid.func <- sigmoid.integral.by.normal(x, estimation.result$param$mu, inv.beta)
  
  pred.label <- apply(rmultinom(n, size=1, prob=estimation.result$mean.estimator$ratio), 2, which.max)
  # pred.label <- apply(estimation.result$latent$latent.variable, 1, which.max)
  pred.prob <- diag(pred.sigmoid.func[,pred.label])
  return(list(pred.prob = pred.prob, pred.sigmoid = pred.sigmoid.func))
}

prediction.logistic.mixture.prob <- function(x, parameter.num=10000, estimation.result){
  library(MASS)
  ratio <- estimation.result$mean.estimator$ratio
  n <- dim(x)[1]
  M <- dim(x)[2]
  K <- length(ratio)
  
  posterior.weight <- array(0, dim=c(parameter.num, M,K))
  for(k in 1:K){
    posterior.weight[,,k] <- mvrnorm(n=parameter.num, mu=estimation.result$param$mu[,k], Sigma=solve(estimation.result$param$beta[,,k]))
  }
  
  pred.sigmoid.func <- monte.carlo.integral.sigmoid(x, posterior.weight)
  
  pred.label <- apply(rmultinom(n, size=1, prob=estimation.result$mean.estimator$ratio), 2, which.max)
  # pred.label <- apply(estimation.result$latent$latent.variable, 1, which.max)
  pred.prob <- diag(pred.sigmoid.func[,pred.label])
  return(list(pred.prob = pred.prob, pred.sigmoid = pred.sigmoid.func))
}

prediction.latent.posterior.logistic.mixture <- function(x, y, estimation.result, seed = 1, parameter.num=10000){
  library(MASS)
  ratio <- estimation.result$mean.estimator$ratio
  n <- dim(x)[1]
  M <- dim(x)[2]
  K <- length(ratio)
  
  post.param <- generate.param.logistic.mixture(parameter.num, K, M, seed=seed, hyperparameter = estimation.result$param)
  post.latent.prob <- matrix(0, nrow=n, ncol=K)
  
  for(l in 1:parameter.num){
    l.post.param <- list(ratio = as.numeric(post.param$ratio[l,]), weight = post.param$weight[l,,] )
    post.latent.prob <- post.latent.prob + dlatent.posterior.logistic.mixture(x, y, l.post.param) / parameter.num
  }
  return(post.latent.prob)
}

monte.carlo.integral.sigmoid <- function(x,post.weight){
  parameter.num <- dim(post.weight)[1]
  M <- dim(post.weight)[2]
  K <- dim(post.weight)[3]
  
  test.num <- dim(x)[1]
  
  pred.sigmoid.partial <- array(0, dim=c(parameter.num, test.num, K))
  for(i in 1:parameter.num){
    pred.sigmoid.partial[i,,] <- sigmoid(x, as.matrix(post.weight[i,,], nrow=M, ncol=K))
  }  
  pred.sigmoid.x <- apply(pred.sigmoid.partial, c(2,3), mean)
  
  return(pred.sigmoid.x)
}

estimation.LRMM.LvA.Minibatch <- function(total.size, batch.size,
                                              K, M,
                                              last.training.x, last.training.y,
                                              hyperparameter.list,
                                              init.phi = 1, init.df = M+5, init.Sigma = diag(M),
                                              iteration = 2000,
                                              data.seed = 1,
                                              learning.seed = 1,
                                              iteration.diff = 20,
                                              tol.diff = 1e-3,
                                              tol = 1e-5,
                                              trace.on = F
                                              ){
  ### This function caculates an approximated  posterior distribution by LVA minibatch learning.
  phi <- hyperparameter.list$phi
  beta <- hyperparameter.list$beta
  
  # initialize for estimated parameters
  set.seed(learning.seed)
  est.phi <- numeric(K)
  est.mu <- matrix(0, nrow=M, ncol=K)
  est.beta <- array(0, dim=c(M,M,K))
  pred.beta <- array(0, dim=c(M,M,K))
  pred.mu <- matrix(0, nrow=M, ncol=K)
  
  init.phi.parameters <- rep(init.phi,K)
  est.phi <- batch.size * rdirichlet(init.phi.parameters)
  
  init.df.parameters <- init.df
  init.Sigma.parameters <- diag(M)
  est.beta <- rWishart(K,df=init.df.parameters, Sigma=init.Sigma.parameters)
  inv.est.beta <- array(0,dim=c(M,M,K))
  est.nu <- matrix(0, nrow=M, ncol=K)
  est.mu <- matrix(0, nrow=M, ncol=K)
  est.mu.cov <- array(0, dim=c(M,M,K))
  for(k in 1:K){
    inv.est.beta[,,k] <- solve(est.beta[,,k])
    est.mu[,k] <- mvrnorm(n=1, mu=rep(0,M), Sigma=inv.est.beta[,,k])
    est.mu.cov[,,k] <- inv.est.beta[,,k] + tcrossprod(est.mu[,k])
  }
  
  prev.phi <- numeric(K) + phi
  prev.beta <- array(rep(beta * diag(M),K), dim=c(M,M,K))
  prev.nu <- matrix(0, nrow=M, ncol=K)
  
  # initialize for latent 
  est.h.xi <- matrix(0, nrow=batch.size, ncol=K)
  est.z.xi <- matrix(0, nrow=batch.size, ncol=K)
  for(i in 1:batch.size){
    est.z.xi[i,] <- rdirichlet(rep(0.1,K))
  }
  
  # initialize for auxiliary variables
  est.g.eta <- matrix(0, nrow=batch.size, ncol=K)
  for(i in 1:batch.size){
    est.g.eta[i,] <- abs(rnorm(K))
  }
  est.v.eta <- -tanh(sqrt(est.g.eta)/2)/(4*sqrt(est.g.eta))
  
  energy.trace <- numeric(iteration*iteration.diff)
  energy.trace.ind <- 1
  
  energy.step1 <- numeric(iteration)-1
  set.seed(data.seed)
  ###### learning part
  for(ite1 in 1:iteration){
    D <- 1
    # D <- ite1/iteration
    
    ### data generation
    current.training.x <- matrix(0, nrow=batch.size, ncol=M)
    base.traininig.x <- runif(batch.size, min = x.range[1], max = x.range[2])
    current.training.y <- numeric(batch.size)
    if(ite1 < iteration){
      ## input
      for(j in 1:M){
        current.training.x[,j] <- base.traininig.x^(j-1)
      }

      ## output
      current.true.label <- apply(rmultinom(batch.size, size=1, prob=true.ratio), 2, which.max)
      prob.y <- 1/(1+exp(-rowSums(current.training.x * t(true.weight[,current.true.label]))))
      current.training.y <- as.numeric(runif(batch.size) < prob.y)
    }else{
      current.training.x <- last.training.x
      current.training.y <- last.training.y
    }
    
    in.out.matrix <- matrix(rep((current.training.y - 0.5),M), nrow=batch.size, ncol=M) * current.training.x
    
    # update auxilirary vairables
    energy.step2 <- numeric(iteration.diff)-1
    ite2 <- 1
    for(ite2 in 1:iteration.diff){
      ## update auxiliary variables
      for(k in 1:K){
        est.g.eta[,k] <- rowSums((current.training.x %*% est.mu.cov[,,k]) * current.training.x)
      }
      est.sq.g.eta <- sqrt(est.g.eta)  
      est.v.eta <- -tanh(est.sq.g.eta/2)/(4*est.sq.g.eta)
      
      # update latent variables
      for(k in 1:K){
        est.h.xi[,k] <- digamma(est.phi[k]) - digamma(sum(est.phi)) +
          in.out.matrix %*% est.mu[,k] - log(2*cosh(est.sq.g.eta[,k]/2))
      }
      max.est.h.xi <- apply(est.h.xi,1,max)
      exp.norm.est.h.xi <- exp(est.h.xi - max.est.h.xi)
      est.z.xi <-  exp.norm.est.h.xi / matrix(rep(rowSums(exp.norm.est.h.xi),K),nrow=batch.size,ncol=K) 
      
      ## update estimated parameters
      # pred.phi <- colSums(est.z.xi)
      est.phi <- prev.phi + D*colSums(est.z.xi)
      for(k in 1:K){
        extend.est.v.eta <- -2 * matrix(rep(est.v.eta[,k],M),nrow=batch.size,ncol=M)
        extend.est.z.xi <- matrix(rep(est.z.xi[,k],M), nrow=batch.size, ncol=M)
        
        est.beta[,,k] <- prev.beta[,,k] + D*crossprod(current.training.x, (extend.est.z.xi * extend.est.v.eta) * current.training.x)
        inv.est.beta[,,k] <- solve(est.beta[,,k])
        est.nu[,k] <- prev.nu[,k] + D*colSums(extend.est.z.xi * in.out.matrix)
        est.mu[,k] <- inv.est.beta[,,k] %*% est.nu[,k]
        
        est.mu.cov[,,k] <- inv.est.beta[,,k] + tcrossprod(est.mu[,k])
      }        
      
      ## update energy
      ## Since the objective function is very vague in online learning,
      ## We consider two version of objective function
      ## 1. F_{step+1} = F(x_{step+1})
      ## 2. F_{step+1} = F_{step} + F(x_{step+1})
      tilde.energy <- -sum(log( rowSums( exp(est.h.xi - max.est.h.xi ) ) ) + max.est.h.xi) + sum(est.z.xi * est.v.eta * est.g.eta) + 
        sum(est.z.xi * (matrix(rep(digamma(est.phi),batch.size), nrow=batch.size, ncol=K , byrow = T)  - digamma(sum(est.phi)) + in.out.matrix %*% est.mu ) ) 
      energy.joint.dist <- lgamma(sum(est.phi)) -lgamma(K*phi) - sum(lgamma(est.phi)) + K*lgamma(phi)
      for(k in 1:K){
        energy.joint.dist <- energy.joint.dist + determinant(est.beta[,,k]/(2*pi), logarithm=T)$modulus[1]/2 - t(est.mu[,k]) %*% est.beta[,,k] %*% est.mu[,k]/2
      }
      energy.joint.dist <- energy.joint.dist - K*determinant(beta*diag(M)/(2*pi), logarithm=T)$modulus[1]/2
      
      energy.step2[ite2] <- tilde.energy + energy.joint.dist
      if(ite2 > 1){
        diff.energy <- energy.step2[ite2] - energy.step2[ite2-1]
        # print(diff.energy)
        if(abs(diff.energy) < tol.diff){
          break
        }
      }
      # ite2 <- ite2 + 1
      
      energy.trace[energy.trace.ind] <- energy.step2[ite2]
      energy.trace.ind <- energy.trace.ind + 1
    }
    
    ## update previous post hyperparameters
    prev.phi <- est.phi
    prev.beta <- est.beta
    prev.nu <- est.nu
    
    if(ite2 == 1){
      energy.step1[ite1] <- energy.step2
    }else{
      energy.step1[ite1] <- energy.step2[ite2-1]
    }
    
    if(trace.on == T){
      print(energy.step1[ite1])
    }
    # ite1 <- ite1 + 1
  }
  
  return(list(param = list(phi = est.phi, beta=est.beta, mu=est.mu),
              mean.estimator = list(ratio=est.phi/sum(est.phi), weight = est.mu),
              latent.variable = est.z.xi,
              energy.trace = energy.trace))
}

LRMM.EM <- function(x,y,K,
                   iteration = 1000, restart = 1,
                   learning.seed = 1, tol = 1e-5,
                   init.phi = 1, init.mean = 0, init.sd = 1,
                   stan.filename = "LMM.stan"){
  ### This function calculates a local MLE estimator for LRMM.
  ### Since M-step has no closed form, we adopt quasi-newton method implemented by stan.
  
  library(rstan)
  options(mc.cores = parallel::detectCores())
  rstan_options(auto_write = TRUE)
  set.seed(learning.seed)
  n <- nrow(x)
  M <- ncol(x)

  est.logLik <- -Inf
  est.ratio <- numeric(K)
  est.weight <- matrix(0, nrow=M, ncol=K)
  est.z.xi <- matrix(0, nrow=n, ncol=K)
  
  lmm.stan.model <- stan_model(file=stan.filename)
  lmm.stan.data <- list(n = n, M = M, K = K, x = x, y = y)
  for(res in 1:restart){
    candidate.ratio <- numeric(K)
    candidate.weight <- matrix(0, nrow=M, ncol=K)
    
    ## initialization
    candidate.ratio <- rdirichlet(rep(init.phi,K))
    candidate.weight <- matrix(rnorm(M*K, mean = init.mean, sd = init.sd), nrow=M, ncol=K)
    
    prev.ratio <- numeric(K)
    prev.weight <- matrix(0, nrow=M, ncol=K)
    prev.z.xi <- matrix(0, nrow=n, ncol=K)
    prev.logLik <- -Inf
    
    ## EM algorithm
    candidate.logLik.trace <- numeric(iteration)-Inf
    for(ite in 1:iteration){
      ## E-step
      candidate.h.xi <- matrix( rep(log(candidate.ratio),n), nrow=n, ncol=K , byrow = T) + ((y-0.5)*x) %*% candidate.weight-log(2*cosh(x %*% candidate.weight/2))
      max.candidate.h.xi <- apply(candidate.h.xi,1,max)
      norm.candidate.h.xi <- candidate.h.xi - max.candidate.h.xi
      candidate.z.xi <-  exp(norm.candidate.h.xi)/matrix(rep(rowSums( exp(norm.candidate.h.xi) ),K),nrow=n,ncol=K)        
      
      ## loglike
      candidate.logLik <- sum(log( rowSums(exp(norm.candidate.h.xi)) ) + max.candidate.h.xi)
      candidate.logLik.trace[ite] <- candidate.logLik
      if(is.nan(candidate.logLik) || is.infinite(candidate.logLik) || abs(candidate.logLik - prev.logLik) < tol){
        candidate.ratio <- prev.ratio
        candidate.weight <- prev.weight
        candidate.z.xi <- prev.z.xi
        candidate.logLik <- prev.logLik
        break
      }      
      
      ## M-step
      candidate.ratio <- colSums(candidate.z.xi) / n
      lmm.stan.data$z_xi <- candidate.z.xi
      stan.opt <- optimizing(object = lmm.stan.model, data = lmm.stan.data)
      candidate.weight <- matrix(stan.opt$par, nrow = M, ncol=K)
      
      prev.ratio <- candidate.ratio
      prev.weight <- candidate.weight
      prev.z.xi <- candidate.z.xi
      prev.logLik <- candidate.logLik
    }
    candidate.logLik.trace <- candidate.logLik.trace[-((ite+1):iteration)]
    # inf.ind <- which(is.infinite(candidate.logLik.trace))
    # candidate.logLik.trace <- candidate.logLik.trace[-inf.ind]
    
    if(est.logLik < candidate.logLik){
      est.logLik <- candidate.logLik
      est.logLik.trace <- candidate.logLik.trace
      est.ratio <- candidate.ratio
      est.weight <- candidate.weight
      est.z.xi <- candidate.z.xi
    }
    print(paste(as.character(res), "th restart is finished, logLik:",as.character(candidate.logLik),sep=""))
  }  

  return(list(ratio = est.ratio, weight = est.weight, est.z.xi = est.z.xi, logLik.trace = est.logLik.trace, logLik = est.logLik))
}

prediction.logistic.mixture.em <- function(x,param){
  n <- nrow(x)
  K <- length(param$ratio)
  
  pred.prob <- rowSums( matrix(rep(param$ratio,test.num),nrow=n, ncol=K, byrow=T) * 
                          1/(1+exp(-x %*% param$weight)) )
  return(pred.prob)
  
}

pr.generalization.error <- function(test.x, test.y, pred.func){
  pred.prob <- pred.func(test.x)
  pred.loss <- -mean(test.y * log(pred.prob) + (1-test.y) * log(1 - pred.prob))
  return(pred.loss)
}

pr.clustering.error <- function(est.latent.prob, true.latent){
  library(gtools)
  n <- nrow(est.latent.prob)
  K <- ncol(est.latent.prob)
  
  max.cluster <- apply(est.latent.prob,1,which.max)
  prob.cluster.vector <- t(apply(est.latent.prob, 1, function(x) rmultinom(1,1,prob=x)))
  
  true.label.vector <- matrix(0, nrow=n, ncol=K)
  max.cluster.vector <- matrix(0, nrow=n, ncol=K)
  for(i in 1:n){
    max.cluster.vector[i,max.cluster[i]] <- 1
    true.label.vector[i,true.latent[i]] <- 1
  }
  
  ## calculate max correctness
  perm <- permutations(n=K, r=K)
  max.01loss <- Inf
  prob.01loss <- Inf  
  
  max.perm <- NULL
  
  prob.perm <- NULL
  
  for(i in 1:nrow(perm)){
    # max.correctness.lva <- 
    cond <- true.label.vector == max.cluster.vector[,perm[i,]]
    candidate.max.01loss <- n - sum(rowSums(abs(cond)) == K)
    
    cond <- true.label.vector == prob.cluster.vector[,perm[i,]]
    candidate.prob.01loss <- n - sum(rowSums(abs(cond)) == K)
    
    if(candidate.max.01loss < max.01loss){
      max.perm <- perm[i,]
      max.01loss <- candidate.max.01loss
    }
    
    if(candidate.prob.01loss < prob.01loss){
      prob.perm <- perm[i,]
      prob.01loss <- candidate.prob.01loss
    }
  } 
  return(list(max.loss = max.01loss/n, prob.loss = prob.01loss/n))
}